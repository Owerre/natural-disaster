{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set font scale and style\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Pyspark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import sql, SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data size: (15901, 43)\n",
      "+----+----+---------+-------------------+------------+----------+-----------+-----------+--------------+\n",
      "| seq|year|continent|      disaster type|total deaths|no injured|no affected|no homeless|total affected|\n",
      "+----+----+---------+-------------------+------------+----------+-----------+-----------+--------------+\n",
      "|9002|1900|   Africa|            Drought|       11000|      null|       null|       null|          null|\n",
      "|9001|1900|     Asia|            Drought|     1250000|      null|       null|       null|          null|\n",
      "|  12|1902| Americas|         Earthquake|        2000|      null|       null|       null|          null|\n",
      "|   3|1902| Americas|  Volcanic activity|        1000|      null|       null|       null|          null|\n",
      "|  10|1902| Americas|  Volcanic activity|        6000|      null|       null|       null|          null|\n",
      "|   6|1903| Americas|Mass movement (dry)|          76|        23|       null|       null|            23|\n",
      "|  12|1903|   Africa|  Volcanic activity|          17|      null|       null|       null|          null|\n",
      "|   3|1904|     Asia|              Storm|        null|      null|       null|       null|          null|\n",
      "|   5|1905| Americas|Mass movement (dry)|          18|        18|       null|       null|            18|\n",
      "|   3|1905|     Asia|         Earthquake|       20000|      null|       null|       null|          null|\n",
      "|  14|1906| Americas|         Earthquake|       20000|      null|       null|       null|          null|\n",
      "|   2|1906| Americas|         Earthquake|         400|      null|       null|       null|          null|\n",
      "|  23|1906|   Europe|              Flood|           6|      null|       null|       null|          null|\n",
      "|  24|1906|   Europe|              Flood|        null|      null|       null|       null|          null|\n",
      "|  15|1906|     Asia|              Storm|       10000|      null|       null|       null|          null|\n",
      "|   6|1907|     Asia|         Earthquake|       12000|      null|       null|       null|          null|\n",
      "|   1|1907|     Asia|           Epidemic|     1300000|      null|       null|       null|          null|\n",
      "|  11|1908| Americas|Mass movement (dry)|          33|      null|       null|       null|          null|\n",
      "|  10|1909|     Asia|              Storm|         172|      null|       null|       null|          null|\n",
      "|  13|1909|     Asia|              Storm|        null|      null|       null|       null|          null|\n",
      "+----+----+---------+-------------------+------------+----------+-----------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = spark.read.csv('../data/emdat_public_raw.csv', inferSchema = True, header = True)\n",
    "\n",
    "df = df.toDF(*[c.lower() for c in df.columns]) # column names in lower case\n",
    "\n",
    "print(\"Data size:\", (df.count(), len(df.columns))) # print data size\n",
    "\n",
    "cols = ['seq','year', 'continent', 'disaster type', 'total deaths', 'no injured','no affected','no homeless', 'total affected'\n",
    "]\n",
    "\n",
    "df.select(cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- dis no: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- seq: integer (nullable = true)\n |-- disaster group: string (nullable = true)\n |-- disaster subgroup: string (nullable = true)\n |-- disaster type: string (nullable = true)\n |-- disaster subtype: string (nullable = true)\n |-- disaster subsubtype: string (nullable = true)\n |-- event name: string (nullable = true)\n |-- entry criteria: string (nullable = true)\n |-- country: string (nullable = true)\n |-- iso: string (nullable = true)\n |-- region: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- location: string (nullable = true)\n |-- origin: string (nullable = true)\n |-- associated dis: string (nullable = true)\n |-- associated dis2: string (nullable = true)\n |-- ofda response: string (nullable = true)\n |-- appeal: string (nullable = true)\n |-- declaration: string (nullable = true)\n |-- aid contribution: integer (nullable = true)\n |-- dis mag value: integer (nullable = true)\n |-- dis mag scale: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- local time: string (nullable = true)\n |-- river basin: string (nullable = true)\n |-- start year: integer (nullable = true)\n |-- start month: integer (nullable = true)\n |-- start day: integer (nullable = true)\n |-- end year: integer (nullable = true)\n |-- end month: integer (nullable = true)\n |-- end day: integer (nullable = true)\n |-- total deaths: integer (nullable = true)\n |-- no injured: integer (nullable = true)\n |-- no affected: integer (nullable = true)\n |-- no homeless: integer (nullable = true)\n |-- total affected: integer (nullable = true)\n |-- reconstruction costs ('000 us$): integer (nullable = true)\n |-- insured damages ('000 us$): integer (nullable = true)\n |-- total damages (us$): integer (nullable = true)\n |-- cpi: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "source": [
    "# Select relevant columns for analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----+---------+----------------+-------------------+-----------+\n|      country|year|continent|          region|      disaster_type|start_month|\n+-------------+----+---------+----------------+-------------------+-----------+\n|   Cabo Verde|1900|   Africa|  Western Africa|            Drought|       null|\n|        India|1900|     Asia|   Southern Asia|            Drought|       null|\n|    Guatemala|1902| Americas| Central America|         Earthquake|          4|\n|    Guatemala|1902| Americas| Central America|  Volcanic activity|          4|\n|    Guatemala|1902| Americas| Central America|  Volcanic activity|         10|\n|       Canada|1903| Americas|Northern America|Mass movement (dry)|          4|\n|Comoros (the)|1903|   Africa|  Eastern Africa|  Volcanic activity|       null|\n|   Bangladesh|1904|     Asia|   Southern Asia|              Storm|         11|\n|       Canada|1905| Americas|Northern America|Mass movement (dry)|          8|\n|        India|1905|     Asia|   Southern Asia|         Earthquake|          4|\n|        Chile|1906| Americas|   South America|         Earthquake|          8|\n|     Colombia|1906| Americas|   South America|         Earthquake|          1|\n|      Belgium|1906|   Europe|  Western Europe|              Flood|          5|\n|      Belgium|1906|   Europe|  Western Europe|              Flood|          4|\n|    Hong Kong|1906|     Asia|    Eastern Asia|              Storm|          9|\n|        China|1907|     Asia|    Eastern Asia|         Earthquake|         10|\n|        India|1907|     Asia|   Southern Asia|           Epidemic|       null|\n|       Canada|1908| Americas|Northern America|Mass movement (dry)|          4|\n|   Bangladesh|1909|     Asia|   Southern Asia|              Storm|         10|\n|   Bangladesh|1909|     Asia|   Southern Asia|              Storm|         12|\n+-------------+----+---------+----------------+-------------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Select columns\n",
    "cols = ['iso','country', 'region', 'continent',  'year',  'disaster type','latitude', 'longitude','local time',  'start month','total deaths', 'no injured', 'no affected', 'no homeless', 'total affected','total damages (us$)', 'cpi'\n",
    "]\n",
    "df_sel = df.select(cols)\n",
    "\n",
    "# Add underscores between column names\n",
    "df_sel = df_sel.toDF(*[c.replace(' ', '_') for c in df_sel.columns]) \n",
    "\n",
    "# create table for sql query\n",
    "df_sel.createOrReplaceTempView(\"raw_table\") \n",
    "\n",
    "# Show data\n",
    "cols =['country', 'year', 'continent', 'region', 'disaster_type', 'start_month']\n",
    "df_sel.select(cols).show()"
   ]
  },
  {
   "source": [
    "# Data types and missing values\n",
    "Some variables have missing values.  Although they will not affect exploratory data analysis, we will impute them during modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----+------------+-------------+----------+-----------+-----------+--------------+\n|country| year|total_deaths|disaster_type|no_injured|no_affected|no_homeless|total_affected|\n+-------+-----+------------+-------------+----------+-----------+-----------+--------------+\n|  15901|15901|       11281|        15901|      3832|       9044|       2414|         11421|\n+-------+-----+------------+-------------+----------+-----------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_missing = df_sel.agg(*[count(c).alias(c) for c in df_sel.columns])\n",
    "\n",
    "cols = ['country', 'year', 'total_deaths','disaster_type', 'no_injured','no_affected','no_homeless', 'total_affected'\n",
    "]\n",
    "df_missing.select(cols).show()"
   ]
  },
  {
   "source": [
    "# Categorical attributes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+\n|local_time|count|\n+----------+-----+\n|      null|14804|\n|     21:48|    6|\n|     20:38|    5|\n|     15:00|    5|\n|     16:30|    5|\n|     05:15|    5|\n|     12:55|    4|\n|     15:30|    4|\n|     10:26|    4|\n|     20:28|    4|\n|     13:35|    4|\n|     12:19|    4|\n|     04:47|    4|\n|     06:42|    4|\n|     20:06|    4|\n|     03:00|    3|\n|     12:15|    3|\n|     13:39|    3|\n|     20:18|    3|\n|     16:40|    3|\n+----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('local_time')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from time\n",
    "df_sel = df_sel.withColumn('local_hour', split(col('local_time'), \":\").getItem(0))\n",
    "df_sel = df_sel.withColumn('local_hour', split(col('local_hour'), \"-\").getItem(0))\n",
    "df_sel = df_sel.withColumn('local_hour', floor(col('local_hour'))) # get nearest integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+\n|local_hour|count|\n+----------+-----+\n|        95|    1|\n|        88|    1|\n|        39|    1|\n|        22|   32|\n|        23|   37|\n|         7|   37|\n|         0|   37|\n|        14|   39|\n|        15|   40|\n|         9|   40|\n|        19|   41|\n|        10|   41|\n|        11|   41|\n|         8|   43|\n|        17|   44|\n|        16|   44|\n|        18|   45|\n|        13|   47|\n|         2|   47|\n|        21|   48|\n+----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('local_hour')\\\n",
    "    .count()\\\n",
    "    .sort(asc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "\n",
    "def replace_hour(x: pd.Series) -> pd.Series:\n",
    "    \"\"\" \n",
    "    pandas udf for replacing values with other values\n",
    "    \"\"\"\n",
    "    param_dict = {39:3, 88:8, 95:5}\n",
    "    return x.replace(param_dict)\n",
    "\n",
    "# Replace inconsistent hours\n",
    "df_sel = df_sel.withColumn('local_hour', replace_hour(df_sel['local_hour']))"
   ]
  },
  {
   "source": [
    "## 2. Month"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-----+\n|start_month|count|\n+-----------+-----+\n|          1| 1759|\n|          8| 1630|\n|          7| 1599|\n|          9| 1441|\n|         10| 1283|\n|          6| 1279|\n|          5| 1200|\n|         12| 1123|\n|          4| 1098|\n|         11| 1052|\n|          2| 1029|\n|          3| 1023|\n|       null|  385|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('start_month')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "source": [
    "## 3. Country, Region, and Continent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|             country|count|\n+--------------------+-----+\n|United States of ...| 1064|\n|               China|  971|\n|               India|  740|\n|   Philippines (the)|  663|\n|           Indonesia|  568|\n|               Japan|  372|\n|          Bangladesh|  354|\n|              Mexico|  285|\n|              Brazil|  248|\n|Iran (Islamic Rep...|  248|\n|           Australia|  247|\n|            Viet Nam|  245|\n|            Pakistan|  230|\n|              Turkey|  203|\n|         Afghanistan|  203|\n|            Colombia|  203|\n|                Peru|  200|\n|              France|  181|\n|               Italy|  171|\n|Russian Federatio...|  168|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('country')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|              region|count|\n+--------------------+-----+\n|       Southern Asia| 2037|\n|  South-Eastern Asia| 1920|\n|        Eastern Asia| 1825|\n|       South America| 1268|\n|    Northern America| 1211|\n|      Eastern Africa| 1145|\n|     Central America|  808|\n|      Western Africa|  794|\n|     Southern Europe|  642|\n|           Caribbean|  620|\n|      Eastern Europe|  534|\n|      Western Europe|  517|\n|        Western Asia|  491|\n|       Middle Africa|  423|\n|     Northern Africa|  339|\n|Australia and New...|  322|\n|           Melanesia|  256|\n|     Northern Europe|  209|\n|     Southern Africa|  207|\n|        Central Asia|  138|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('region')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+-----+\n|continent|count|\n+---------+-----+\n|     Asia| 6411|\n| Americas| 3907|\n|   Africa| 2908|\n|   Europe| 1962|\n|  Oceania|  713|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('continent')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "source": [
    "## 4. Disaster type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|       disaster_type|count|\n+--------------------+-----+\n|               Flood| 5447|\n|               Storm| 4434|\n|          Earthquake| 1534|\n|            Epidemic| 1496|\n|           Landslide|  768|\n|             Drought|  758|\n| Extreme temperature|  601|\n|            Wildfire|  456|\n|   Volcanic activity|  259|\n|  Insect infestation|   96|\n| Mass movement (dry)|   48|\n|              Impact|    1|\n|     Animal accident|    1|\n|Glacial lake outb...|    1|\n|                 Fog|    1|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('disaster_type')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "source": [
    "# Add year in decade attribute\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel = df_sel.withColumn('year_in_decade', (floor(col('year')/10))*10)"
   ]
  },
  {
   "source": [
    "# Incorrect data entries in longitude and latitude"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+\n|longitude|latitude|\n+---------+--------+\n|  78.46 W|  1.51 N|\n|  31.15 E| 30.03 N|\n|  58.00 W| 48.60 N|\n|  71.40 W| 35.28 S|\n|  23.44 E| 38.00 N|\n| 121.70 E|  8.30 S|\n| 104.06 E| 30.37 N|\n|  78.30 W|  0.14 S|\n|  26.76 E| 45.77 N|\n| 119.41 W| 34.25 N|\n|  27.10 E| 38.25 N|\n|  32.56 E|  2.31 S|\n|  56.57 E| 27.69 N|\n|   7.25 E| 43.70 N|\n|  87.72 W| 14.53 N|\n|  71.40 E| 34.01 N|\n|   5.31 E| 50.37 N|\n|  70.28 E| 34.26 N|\n|  25.57 E| 43.12 N|\n|  62.42 W|  3.58 S|\n+---------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "    longitude, \n",
    "    latitude \n",
    "from raw_table\n",
    "where longitude like '%W%' \n",
    "or longitude like '%E%'\n",
    "or longitude like '%N%'\n",
    "or longitude like '%S%'\n",
    "or latitude like '%W%' \n",
    "or latitude like '%E%'\n",
    "or latitude like '%N%'\n",
    "or latitude like '%S%'\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the strings from numbers\n",
    "df_sel = df_sel.withColumn('longitude', regexp_replace('longitude', '[EW]', ''))\n",
    "df_sel = df_sel.withColumn('latitude', regexp_replace('latitude', '[NS]', ''))\n",
    "\n",
    "# create table for sql query\n",
    "df_sel.createOrReplaceTempView(\"raw_table2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+\n|longitude|latitude|\n+---------+--------+\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cross-check if strings were removed\n",
    "query = \"\"\"\n",
    "select \n",
    "    longitude, \n",
    "    latitude \n",
    "from raw_table2\n",
    "where longitude like '%W%' \n",
    "or longitude like '%E%'\n",
    "or longitude like '%N%'\n",
    "or longitude like '%S%'\n",
    "or latitude like '%W%' \n",
    "or latitude like '%E%'\n",
    "or latitude like '%N%'\n",
    "or latitude like '%S%'\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "source": [
    "# Add number of occurrence per year"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+--------------+---------+-------------+\n|year|year_in_decade|continent|no_occurrence|\n+----+--------------+---------+-------------+\n|1900|          1900|   Africa|            7|\n|1900|          1900|     Asia|            7|\n|1902|          1900| Americas|           10|\n|1902|          1900| Americas|           10|\n|1902|          1900| Americas|           10|\n|1903|          1900| Americas|           12|\n|1903|          1900|   Africa|           12|\n|1904|          1900|     Asia|            4|\n|1905|          1900| Americas|            8|\n|1905|          1900|     Asia|            8|\n|1906|          1900| Americas|           13|\n|1906|          1900| Americas|           13|\n|1906|          1900|   Europe|           13|\n|1906|          1900|   Europe|           13|\n|1906|          1900|     Asia|           13|\n|1907|          1900|     Asia|            3|\n|1907|          1900|     Asia|            3|\n|1908|          1900| Americas|            3|\n|1909|          1900|     Asia|           17|\n|1909|          1900|     Asia|           17|\n+----+--------------+---------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_final = spark.sql(\"\"\"\n",
    "select \n",
    "    t1.*, t2.no_occurrence\n",
    "from raw_table2 as t1\n",
    "left join (\n",
    "    select year, count(*) as no_occurrence from raw_table2\n",
    "group by 1\n",
    ") as t2 on (t1.year = t2.year)\n",
    "\"\"\")\n",
    "df_final.select('year', 'year_in_decade', 'continent','no_occurrence').show()"
   ]
  },
  {
   "source": [
    "# Convert to numerical variables to numeric"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_numeric(df, dont_cols):\n",
    "    \"\"\"\n",
    "    Convert numerical columns to double type\n",
    "    \"\"\"\n",
    "    cols = [x for x in df.columns if x not in dont_cols]\n",
    "    for col in cols:\n",
    "        df = df.withColumn(col, df[col].cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Cast columns not in dont_cols to float\n",
    "dont_cols = ['iso', 'country', 'region', 'continent', 'disaster_type', \n",
    "            'year', 'year_in_decade', 'local_time', 'start_month']\n",
    "df_final = df_to_numeric(df_final, dont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- iso: string (nullable = true)\n |-- country: string (nullable = true)\n |-- region: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- disaster_type: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- local_time: string (nullable = true)\n |-- start_month: integer (nullable = true)\n |-- total_deaths: double (nullable = true)\n |-- no_injured: double (nullable = true)\n |-- no_affected: double (nullable = true)\n |-- no_homeless: double (nullable = true)\n |-- total_affected: double (nullable = true)\n |-- total_damages_(us$): double (nullable = true)\n |-- cpi: double (nullable = true)\n |-- local_hour: double (nullable = true)\n |-- year_in_decade: long (nullable = true)\n |-- no_occurrence: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "source": [
    "# Save cleaned data as csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.coalesce(1)\\\n",
    "      .write.format('spark.csv')\\\n",
    "      .option(\"header\",\"true\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .csv(\"../data/emdat_public_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}